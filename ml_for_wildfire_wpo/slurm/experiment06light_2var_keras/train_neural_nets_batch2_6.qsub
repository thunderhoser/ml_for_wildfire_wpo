#!/bin/bash

#SBATCH --job-name="train_neural_nets_batch2"
#SBATCH --partition="fge"
#SBATCH --account="rda-ghpcs"
#SBATCH --qos="gpuwf"
#SBATCH --nodes=1
#SBATCH --ntasks=8           # 8 tasks per node
#SBATCH --cpus-per-task=2
#SBATCH --ntasks-per-node=8  # 8 GPUs per node
#SBATCH --exclusive
#SBATCH --time=30:00:00
#SBATCH --array=5
#SBATCH --exclude=h28n12
##SBATCH --exclude=h31n03,h28n12
##SBATCH --exclude=h29n01,h26n05,h28n06,h29n07,h30n03,h30n15
#SBATCH --mail-type=BEGIN,END,FAIL
#SBATCH --mail-user=ryan.lagerquist@noaa.gov
#SBATCH --output=train_neural_nets_batch2_%A_%a.out

module load cuda/10.1
conda init
conda activate base

echo `which conda`
echo `which python`
echo `which python3`

PATH=/usr/local/cuda/bin:$PATH
echo $PATH

CODE_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml_for_wildfire_wpo_standalone/ml_for_wildfire_wpo"
TEMPLATE_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml_for_wildfire_models/experiment06light_2var_keras/templates"
TOP_OUTPUT_DIR_NAME="/scratch1/RDARCH/rda-ghpcs/Ryan.Lagerquist/ml_for_wildfire_models/experiment06light_2var_keras"

GFS_DIR_NAME="/scratch2/BMC/gsd-hpcs/Ryan.Lagerquist/ml_for_wildfire_wpo_project/gfs_data/processed/normalized_params_from_2019-2020"
TARGET_DIR_NAME="/scratch2/BMC/gsd-hpcs/Ryan.Lagerquist/ml_for_wildfire_wpo_project/canadian_fwi"
GFS_FORECAST_TARGET_DIR_NAME="/scratch2/BMC/gsd-hpcs/Ryan.Lagerquist/ml_for_wildfire_wpo_project/gfs_data/processed_fwi_forecasts"
TARGET_NORM_FILE_NAME="/scratch2/BMC/gsd-hpcs/Ryan.Lagerquist/ml_for_wildfire_wpo_project/canadian_fwi/z_score_params_2019-2020.nc"

BATCH_SIZES=("08" "08" "08" "08" "08" "16" "16" "16" "16" "16" "24" "24" "24" "24" "24" "32" "32" "32" "32" "32" "08" "08" "08" "08" "08" "16" "16" "16" "16" "16" "24" "24" "24" "24" "24" "32" "32" "32" "32" "32" "08" "08" "08" "08" "08" "16" "16" "16" "16" "16" "24" "24" "24" "24" "24" "32" "32" "32" "32" "32" "08" "08" "08" "08" "08" "16" "16" "16" "16" "16" "24" "24" "24" "24" "24" "32" "32" "32" "32" "32" "08" "08" "08" "08" "08" "16" "16" "16" "16" "16" "24" "24" "24" "24" "24" "32" "32" "32" "32" "32" "08" "08" "08" "08" "08" "16" "16" "16" "16" "16" "24" "24" "24" "24" "24" "32" "32" "32" "32" "32")
DROPOUT_LAYER_COUNTS=("1" "1" "1" "1" "1" "1" "1" "1" "1" "1" "1" "1" "1" "1" "1" "1" "1" "1" "1" "1" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "2" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "3" "4" "4" "4" "4" "4" "4" "4" "4" "4" "4" "4" "4" "4" "4" "4" "4" "4" "4" "4" "4" "5" "5" "5" "5" "5" "5" "5" "5" "5" "5" "5" "5" "5" "5" "5" "5" "5" "5" "5" "5" "6" "6" "6" "6" "6" "6" "6" "6" "6" "6" "6" "6" "6" "6" "6" "6" "6" "6" "6" "6")
DROPOUT_RATES=("0.1" "0.2" "0.3" "0.4" "0.5" "0.1" "0.2" "0.3" "0.4" "0.5" "0.1" "0.2" "0.3" "0.4" "0.5" "0.1" "0.2" "0.3" "0.4" "0.5" "0.1" "0.2" "0.3" "0.4" "0.5" "0.1" "0.2" "0.3" "0.4" "0.5" "0.1" "0.2" "0.3" "0.4" "0.5" "0.1" "0.2" "0.3" "0.4" "0.5" "0.1" "0.2" "0.3" "0.4" "0.5" "0.1" "0.2" "0.3" "0.4" "0.5" "0.1" "0.2" "0.3" "0.4" "0.5" "0.1" "0.2" "0.3" "0.4" "0.5" "0.1" "0.2" "0.3" "0.4" "0.5" "0.1" "0.2" "0.3" "0.4" "0.5" "0.1" "0.2" "0.3" "0.4" "0.5" "0.1" "0.2" "0.3" "0.4" "0.5" "0.1" "0.2" "0.3" "0.4" "0.5" "0.1" "0.2" "0.3" "0.4" "0.5" "0.1" "0.2" "0.3" "0.4" "0.5" "0.1" "0.2" "0.3" "0.4" "0.5" "0.1" "0.2" "0.3" "0.4" "0.5" "0.1" "0.2" "0.3" "0.4" "0.5" "0.1" "0.2" "0.3" "0.4" "0.5" "0.1" "0.2" "0.3" "0.4" "0.5")

batch_size=${BATCH_SIZES[$SLURM_ARRAY_TASK_ID]}
num_dropout_layers=${DROPOUT_LAYER_COUNTS[$SLURM_ARRAY_TASK_ID]}
dropout_rate=${DROPOUT_RATES[$SLURM_ARRAY_TASK_ID]}

template_file_name="${TEMPLATE_DIR_NAME}/batch-size=${batch_size}_num-dropout-layers=${num_dropout_layers}_dropout-rate=${dropout_rate}/model.keras"
output_dir_name="${TOP_OUTPUT_DIR_NAME}/batch-size=${batch_size}_num-dropout-layers=${num_dropout_layers}_dropout-rate=${dropout_rate}"
echo $output_dir_name

# Number of 8-sample batches is 8, 16, 24, or 32 -- corresponding to {1, 2, 3, 4} gradient-accumulation steps, respectively, for optimizer.

python3 -u "${CODE_DIR_NAME}/train_neural_net.py" \
--input_template_file_name="${template_file_name}" \
--output_model_dir_name="${output_dir_name}" \
--inner_latitude_limits_deg_n 17 73 \
--inner_longitude_limits_deg_e 171 -65 \
--outer_latitude_buffer_deg=5 \
--outer_longitude_buffer_deg=5 \
--gfs_predictor_field_names "temperature_kelvins" "specific_humidity_kg_kg01" "geopotential_height_m_asl" "u_wind_m_s01" "v_wind_m_s01" "temperature_2m_agl_kelvins" "specific_humidity_2m_agl_kg_kg01" "u_wind_10m_agl_m_s01" "v_wind_10m_agl_m_s01" "accumulated_precip_metres" "volumetric_soil_moisture_fraction_0to10cm" "snow_depth_metres" \
--gfs_pressure_levels_mb 700 900 \
--gfs_predictor_lead_times_hours 0 48 \
--gfs_normalization_file_name="" \
--era5_constant_file_name="/scratch2/BMC/gsd-hpcs/Ryan.Lagerquist/ml_for_wildfire_wpo_project/era5_constants.nc" \
--era5_normalization_file_name="/scratch2/BMC/gsd-hpcs/Ryan.Lagerquist/ml_for_wildfire_wpo_project/z_score_params_era5_constants.nc" \
--era5_constant_predictor_field_names "subgrid_orography_angle_sine" "subgrid_orography_angle_cosine" "subgrid_orography_angle_anisotropy" "geopotential_m2_s02" "subgrid_orography_angle_slope" "filtered_subgrid_orography_stdev_metres" "resolved_orography_stdev_metres" \
--target_field_names "fine_fuel_moisture_code" "buildup_index" \
--target_lead_time_days=2 \
--target_lag_times_days 1 2 3 \
--gfs_forecast_target_lead_times_days 1 2 3 \
--target_normalization_file_name="${TARGET_NORM_FILE_NAME}" \
--num_examples_per_batch=2 \
--sentinel_value=-10 \
--gfs_dir_name_for_training="${GFS_DIR_NAME}" \
--target_dir_name_for_training="${TARGET_DIR_NAME}" \
--gfs_forecast_target_dir_name_for_training="${GFS_FORECAST_TARGET_DIR_NAME}" \
--gfs_init_date_limit_strings_for_training "20190101" "20201210" \
--gfs_dir_name_for_validation="${GFS_DIR_NAME}" \
--target_dir_name_for_validation="${TARGET_DIR_NAME}" \
--gfs_forecast_target_dir_name_for_validation="${GFS_FORECAST_TARGET_DIR_NAME}" \
--gfs_init_date_limit_strings_for_validation "20210101" "20211210" \
--num_epochs=1000 \
--num_training_batches_per_epoch=${batch_size} \
--num_validation_batches_per_epoch=8
